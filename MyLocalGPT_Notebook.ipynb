{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My LocalGPT Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the deopendencies\n",
    "! pip install langchain==0.0.267 chromadb==0.4.6 llama-cpp-python==0.1.78 pdfminer.six==20221105 InstructorEmbedding sentence-transformers faiss-cpu huggingface_hub transformers auto-gptq==0.2.2 docx2txt unstructured urllib3==1.26.6 accelerate click flask requests streamlit Streamlit-extras openpyxl\n",
    "! pip install protobuf==3.20.0; sys_platform != 'darwin';\n",
    "! pip install protobuf==3.20.0; sys_platform == 'darwin' and platform_machine != 'arm64';\n",
    "! pip install protobuf==3.20.3; sys_platform == 'darwin' and platform_machine == 'arm64';\n",
    "! pip install bitsandbytes ; sys_platform != 'win32';\n",
    "! pip install bitsandbytes-windows ; sys_platform == 'win32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline, LlamaCpp\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# for document_loaders: https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/\n",
    "from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer, GenerationConfig, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the constants\n",
    "ROOT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "# define folder for storing database of documents\n",
    "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n",
    "\n",
    "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\"\n",
    "\n",
    "# define Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    is_persistent=True\n",
    ")\n",
    "\n",
    "# set the device\n",
    "DEVICE_TYPE = \"cuda\"\n",
    "\n",
    "# default instructor model\n",
    "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
    "\n",
    "# for llama-2 ggml model\n",
    "MODEL_ID = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
    "MODEL_BASENAME = \"llama-2-7b-chat.ggmlv3.q4_0.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ingesting the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "\n",
    "def load_single_document(file_path: str) -> Document:\n",
    "    # loads single document from a file path\n",
    "\n",
    "    loader = None\n",
    "\n",
    "    if file_path.endswith(\".txt\"):\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        loader = PDFMinerLoader(file_path)\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        loader = CSVLoader(file_path)\n",
    "\n",
    "    if loader is None:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "    return loader.load()[0]\n",
    "\n",
    "def load_documents(source_dir: str) -> list[Document]:\n",
    "    # loads all documents from the source documents directory\n",
    "    all_files = os.listdir(source_dir)\n",
    "\n",
    "    # filter out files to only include .txt, .pdf, and .csv files\n",
    "    all_files = [file_path for file_path in all_files if file_path.endswith(\".txt\") or file_path.endswith(\".pdf\") or file_path.endswith(\".csv\")]\n",
    "\n",
    "    return [load_single_document(f\"{source_dir}/{file_path}\") for file_path in all_files]\n",
    "\n",
    "def load_model(device_type, model_id, model_basename):\n",
    "    '''\n",
    "    Select a model on huggingface.\n",
    "\n",
    "    Running this for the first time, will download the model and cache it for future runs.\n",
    "\n",
    "    Args:\n",
    "        device_type: The device type to use for the model. Either \"cpu\", \"cuda\" or \"mps\"\n",
    "        model_id: Identifier of the model to load from HuggingFace's model hub\n",
    "        model_basename: Basename of the model if using quantized models\n",
    "    \n",
    "    Returns:\n",
    "        A HuggingFacePipeline instance\n",
    "    '''\n",
    "    # Select the Model ID and model_basename (if need be)\n",
    "    model_id = model_id\n",
    "    model_basename = model_basename\n",
    "    \n",
    "    if model_basename is not None:\n",
    "        if \".ggml\" in model_basename:\n",
    "            # using Llamacpp for GGML qunatised models\n",
    "            model_path = hf_hub_download(\n",
    "                repo_id=model_id,\n",
    "                filename=model_basename,\n",
    "                resume_download=True\n",
    "            )\n",
    "\n",
    "            max_ctx_size = 2048\n",
    "\n",
    "            # set the arguments\n",
    "            kwargs = {\n",
    "                \"model_path\": model_path,\n",
    "                \"n_ctx\": max_ctx_size,\n",
    "                \"max_tokens\": max_ctx_size\n",
    "            }\n",
    "\n",
    "            if device_type.lower() == \"mps\":\n",
    "                kwargs[\"n_gpu_layers\"] = 1000\n",
    "            elif device_type.lower() == \"cuda\":\n",
    "                kwargs[\"n_gpu_layers\"] = 1000\n",
    "                kwargs[\"n_batch\"] = max_ctx_size\n",
    "            \n",
    "            return LlamaCpp(**kwargs)\n",
    "\n",
    "        else:\n",
    "            # using AutoGPTQForCausalLM for quantized models\n",
    "            \n",
    "            if \".safetensors\" in model_basename:\n",
    "                # remove the \".safetensors\" ending if present\n",
    "                model_basename = model_basename.replace(\".safetensors\", \"\")\n",
    "\n",
    "            # using AutoGPTQ for quantised models\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id,\n",
    "                use_fast=True\n",
    "            )\n",
    "\n",
    "            model = AutoGPTQForCausalLM.from_quantized(\n",
    "                model_id,\n",
    "                model_basename=model_basename,\n",
    "                use_safetensors=True,\n",
    "                trust_remote_code=True,\n",
    "                device=\"cuda:0\",\n",
    "                use_triton=False,\n",
    "                quantize_config=None,\n",
    "            )\n",
    "    \n",
    "    elif device_type.lower() == \"cuda\":\n",
    "        # using AutoModelForCausalLM for full models\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id\n",
    "        )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            # max_memory={0: \"15GB\"} # Uncomment this line with you encounter CUDA out of memory errors\n",
    "        )\n",
    "        model.tie_weights()\n",
    "    \n",
    "    else:\n",
    "        # using Llama Tokenizer\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(\n",
    "            model_id\n",
    "        )\n",
    "\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            model_id\n",
    "        )\n",
    "\n",
    "    # loading configuration from the model to avoid warnings\n",
    "    # Follow this link for more info: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.from_pretrained.returns\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(\n",
    "        model_id\n",
    "    )\n",
    "\n",
    "    # create pipeline for text generation\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=2048,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "    local_llm = HuggingFacePipeline(\n",
    "        pipeline=pipe\n",
    "    )\n",
    "\n",
    "    return local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the vectorstore for the doucment embeddings\n",
    "\n",
    "# load documents and split them into chunks\n",
    "print(f\"Loading documents from {SOURCE_DIRECTORY}\")\n",
    "documents = load_documents(SOURCE_DIRECTORY)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")\n",
    "print(f\"Split the documents into {len(texts)} chunks\")\n",
    "\n",
    "# create embeddings\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": DEVICE_TYPE}\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    texts, \n",
    "    embeddings, \n",
    "    persist_directory=PERSIST_DIRECTORY,\n",
    "    client_settings=CHROMA_SETTINGS,\n",
    "    )\n",
    "db.persist()\n",
    "db = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up the Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(show_resources):\n",
    "    # load the instruction embeddings\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={\"device\": DEVICE_TYPE}\n",
    "    )\n",
    "\n",
    "    # load the vectorstore\n",
    "    db = Chroma(\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        embedding_function=embeddings,\n",
    "        client_settings=CHROMA_SETTINGS\n",
    "    )\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    # set up the prompt template\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    {history}\n",
    "\n",
    "    Question: {question}\n",
    "    Helpul Answer:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"history\", \"question\"],\n",
    "        template=template\n",
    "    )\n",
    "    memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n",
    "\n",
    "\n",
    "    # prepare the LLM\n",
    "    # callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "    # load the LLM for generating Natural Language responses\n",
    "    llm = load_model(\n",
    "        device_type=DEVICE_TYPE,\n",
    "        model_id=MODEL_ID,\n",
    "        model_basename=MODEL_BASENAME\n",
    "    )\n",
    "\n",
    "    # setup langchain pipeline\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\":prompt, \"memory\":memory}\n",
    "    )\n",
    "\n",
    "    # interactive questions and answers loop\n",
    "    while True:\n",
    "        query = input(\"\\nEnter a query: \")\n",
    "        if query == \"exit\":\n",
    "            break\n",
    "\n",
    "        # get answer from the chain\n",
    "        res = qa(query)\n",
    "        answer, docs = res[\"result\"], res[\"source_documents\"]\n",
    "\n",
    "        # print the query and results\n",
    "        print(\"\\n\\n> Question:\")\n",
    "        print(query)\n",
    "\n",
    "        print(\"\\n> Answer:\")\n",
    "        print(answer)\n",
    "\n",
    "        # print the source documents\n",
    "        if show_resources:\n",
    "            # this is a flag used to print relevant resources for answers\n",
    "            print(\"\\n----------------------------------SOURCE DOCUMENTS---------------------------\")\n",
    "            for document in docs:\n",
    "                print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n",
    "                print(document.page_content)\n",
    "            print(\"\\n----------------------------------SOURCE DOCUMENTS---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Chat with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(show_resources=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
